---
title: "AA_Practica3"
author: "Guillermo Gómez"
date: "May 16, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(123)

library(AppliedPredictiveModeling)
transparentTheme(trans = .4)
library(caret)
library(doMC)
```

# Clasificación de dígitos manuscritos
## Observación y comprensión de los datos
```{r}
dtrain = read.csv("data/optdigits_tra.csv",header = FALSE)
dtrain$V65 = factor(dtrain$V65)
nrow(dtrain)
``` 
Tenemos ante nosotros 3823 muestras de dígitos manuscritos ---entre el 0 y el 9---, originalmente era imágenes de 32x32 píxeles que se han convertido a 64 variables, con valores entre 0 y 16 donde cada valor corresponde a la suma de los 16 píxeles adayacentes, 0 para blanco, 1 para negro. Vamos a dibujar en primer lugar la media de los valores.

```{r cars}
colors<-c('white','black')
cus_col<-colorRampPalette(colors=colors)

par(mfrow=c(4,3),pty='s',mar=c(1,1,1,1),xaxt='n',yaxt='n')
all_img<-array(dim=c(10,8*8))
meanDrawing = list()
for(di in 0:9)
{
dpos = (di+1)
all_img[dpos,]<-apply(dtrain[dtrain$V65==di,-65],2,sum)
all_img[dpos,]<-all_img[dpos,]/max(all_img[dpos,])*255
 
z<-array(all_img[dpos,],dim=c(8,8))
z<-z[,8:1] ##right side up
meanDrawing[[dpos]] = z
image(1:8,1:8,z,main=di,col=cus_col(256),ylab = "")
}
```

El hecho de poder distinguir a simple vista los 10 dígitos parece indicar que esta reducción de variables conserva suficiente significancia para distinguir unos números de otros y que aunque algunos sean parecidos ---tomemos el 1 y el 4, o el 5 y el 8 como ejemplo--- poseen características diferenciadoras entre ellos. A simple vista observamos que habrá variables que probablemente podamos eliminar, como son todas las que pertenecen a la primera y última columna pues parecen poco significativas al estar vacías y seguramente algunas casillas que aparecen oscuras en todos los números por igual; más adelante obtendremos estas variables despreciables por métodos estadísticos.

Vamos a tomar ahora dos números al azar, así como 8 de sus características y veamos qué aspecto tienen.

```{r}
sVals = sample(0:9,2)
d = as.data.frame(dtrain[dtrain$V65 == sVals[1] | dtrain$V65 == sVals[2],])

srow = sample(nrow(d),32)
scol = c(sample(ncol(d)-1,8),65)

dp = d[srow,scol]
dp$V65 = factor(dp$V65)

featurePlot(x = dp[,1:8],
            y = dp$V65,
            plot = "pairs",
            ## Add a key at the top
            auto.key = list(columns = 2))

featurePlot(x = dp[, 1:8], 
            y = dp$V65,
            plot = "density", 
            ## Pass in options to xyplot() to 
            ## make it prettier
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(4, 2), 
            auto.key = list(columns = 2))

featurePlot(x = dp[, 1:8], 
            y = dp$V65,
            plot = "box", 
            ## Pass in options to bwplot() 
            scales = list(y = list(relation="free"),
                          x = list(rot = 90)),  
            layout = c(4,2), 
            auto.key = list(columns = 2))
```
# CAmbiar esto!

Vemos a simple vista que hay combinaciones de características con capacidad predictiva mientras que otras no parecen tan fácilmente separables, esto nos da a entender que probablemente seamos capaces de reducir el problema considerablemente, por ejemplo, mientras que V13 y V12 parecen discriminar bien entre el 1 y el 8 ---los números muestreados de ejemplo--- V31, V34 y V50 tienen muy pobre capacidad discriminatoria, al menos entre estos números.

## Preprocesado de datos

### Variables con varianza cercana a 0

Nos interesa ver aquellas variables que tienen siempre o casi siempre el mismo valor, pues como vimos en las imágenes, las bordes y algunos puntos del centro son poco significativos para la predicción y pueden no solo ralentizar el proceso de aprendiaje así como aumentar el número de muestras para la misma confianza, sino que incluso pueden producir resultados indeseables al no estar bien estimada la importancia de estas características por su bajísima frecuencia.

```{r}

nzv = nearZeroVar(dtrain,saveMetrics = TRUE)
nzv[nzv$nzv,]

nzv = nearZeroVar(dtrain)
fdtrain = dtrain[,-nzv]
dim(fdtrain)
dim(dtrain)
```

Si observamos atentamente vemos cómo únicamente ha saleccionado, como ya preveeíamos los bordes de la imagen 1,9,17... el borde izquierdo  y 8,16,24... el borde derecho. Parece un buen comienzo.

Vamos a analizar la correlación entre las variables, que a simple vista parece razonable pensar que no hay pues al haber tan pocos píxeles, la existencia de uno negro no debería condicionar la existencia de otro contiguo, y probablemente menos aún en otro lado del mapa ---de forma uniforme para todas las etiquetas---.

```{r}
nfdtrain = apply(as.matrix(fdtrain),c(1,2),function(v){as.numeric(v)})
#Calculamos la matriz de correlación
descrCor <-  cor(nfdtrain)
#Como es una matriz simétrica, seleccionamos la mitad superior 
highCorr <- sum(abs(descrCor[upper.tri(descrCor)]) > .95)
paste("Número de parejas correlacionadas:", highCorr)

s = seq(0.49,0.99,by = 0.05)
l = c()
for(i in s){
  l = c(l,sum(abs(descrCor[upper.tri(descrCor)]) > i))
}
plot(s,l,type = "l", xlim = rev(range(s)),main = "Correlación de las variables",xlab = "Umbral", ylab = "Número de correlaciones")
```

Efectivamente la correlación es baja, y debemos bajar el umbral hasta casi 0.8 para empezar a encontrarlas en una mínima frecuencia. De momento ignoraremos la correlación, quizás la retomemos en el futuro para ver si afecta positivamente a la capacidad de predicción.

Ahora vamos a normalizar los datos, para ello les restaremos la media y lo dividiremos por la desviación típica; tras eso aplicaremos Análisis de componentes principales para extraer un grupo menor de variables que son combinaciones independientes de nuestras variables originales, después probaremos también sin este método.

# Probar sin method=PCA!!
# Probar con method="BoxCox"
# Qué pasa con el centroid?!

```{r}
prepVals = preProcess(nfdtrain[,-ncol(nfdtrain)],method = c("center", "scale", "pca"))
trainY = factor(nfdtrain[,ncol(nfdtrain)])
trainX = predict(prepVals,nfdtrain[,-ncol(nfdtrain)])
#centroid = classDist(x = trainX,y = trainY,pca = FALSE)
paste("Reducción de características:",ncol(trainX)/(ncol(dtrain)-1))
```

```{r}
centroidDistance = function (X,Y){
  clss = nlevels(Y)
  l = matrix(rep_len(0,clss*ncol(X)),nrow = clss)
  for (i in 1:length(levels(Y))){
    lab = levels(Y)[i]
    
    for (j in 1:ncol(X)){
      l[i,j] = mean(X[Y == lab],j)
    }
  }
  unlist(lapply(1:nrow(X),function(i){dist(t(cbind(X[i,],l[Y[i],])))}))
}
dists = centroidDistance(trainX,trainY)
featurePlot(dists,trainY,plot = "box", 
              ## Pass in options to bwplot() 
              scales = list(y = list(relation="free")),main="Distancia al centroide")

far = tapply(dists,trainY,function(l){length(which(l > mean(l)+2*sd(l)))/length(l)})
print("Porcentaje > 2*desviacion estandar")
t(t(far))
paste("Media:",mean(far))
```

Vemos que hay sobre un 4% de valores a una distancia 2 veces la desviación estándar respecto a la media, esto no debería ser un gran problema en un clasificador KNN, pero en un modelo lineal sí puede ser problemático, luego lo intentaremos solucionar regularizando los datos y aplicando transformaciones no lineales. Vamos a ver algunos de esos valores extremos.

```{r}
c = rep_len(0,10)
for (i in 0:9){c[i+1] = which(dists == max(dists[trainY == i]))}
par(mfrow=c(4,3),pty='s',mar=c(1,1,1,1),xaxt='n',yaxt='n')
for (i in c){
  z = matrix(as.numeric(dtrain[i,-ncol(dtrain)]),nrow = 8)[,8:1]
  image(1:8,1:8,z,col=cus_col(256),ylab = "",main="Instancia")
  image(1:8,1:8,meanDrawing[[trainY[i]]],col=cus_col(256),ylab = "",main="Media")
}
```

Vamos a hacer una evaluación rápida sobre lo que llevamos hasta el momento con regresión logística multinomial con 5-fold y los ajustes por defecto, luego entraremos en detalle sobre esto.

```{r, message=FALSE}
tC = trainControl(method = "cv", number=5)

t_pca = train(x = trainX, y = trainY, method="multinom", trControl = tC)
t_var = train(V65~.,data = fdtrain, method="multinom", trControl = tC)
t_ori = train(V65~.,data = dtrain, method="multinom", trControl = tC)
```
# CAMBIAR GETACCURACY
```{r}
getAccuracy = function(t){
  t$results$Accuracy[length(t$results$Accuracy)]
}
paste("Original:",getAccuracy(t_ori))
paste("Varianza próxima a 0:",getAccuracy(t_var))
paste("PCA:",getAccuracy(t_pca))
paste("#Reduccion:",ncol(trainX)/ncol(dtrain))

#trellis.par.set(caretTheme())
#plot(t_pca) 
```

Vemos que hemos conseguido reducir el número de variables en un 47% sin penalizar el rendimiento.

```{r}
#t_pca = train(x = trainX, y = trainY, method="svmLinear2", trControl = tC)
#t_pca

#prepVals = preProcess(nfdtrain[,-ncol(nfdtrain)],method = c("center", "scale", "YeoJohnson"))
#trainX = predict(prepVals,nfdtrain[,-ncol(nfdtrain)])
#t_pca = train(x = trainX, y = trainY, method="svmLinear2", trControl = tC)
#t_pca

#prepVals = preProcess(nfdtrain[,-ncol(nfdtrain)],method = c("center", "scale", "pca", "YeoJohnson"))
#trainX = predict(prepVals,nfdtrain[,-ncol(nfdtrain)])
registerDoMC(cores = 8)
tC = trainControl(method = "cv", number=5)

#svmGrid = expand.grid(C = c(0.1,1,10), degree =  c(2), scale = c(0.2,0.3,0.4,0.5))
#t_pca = train(x = fdtrain[,-ncol(fdtrain)], y = trainY, method="svmPoly", trControl = tC, preProcess = c("center", "scale", "pca", "YeoJohnson"), tuneGrid = svmGrid)
#svmPoly The final values used for the model were degree = 2, scale = 0.1 and C = 0.25. (0.9866645)

#svmGrid = expand.grid(C = c(0.1,1,10), degree =  c(2), scale = c(0.2,0.3,0.4,0.5))
t_pca = train(x = fdtrain[,-ncol(fdtrain)], y = trainY, method="svmLinear2", trControl = tC, preProcess = c("center", "scale", "pca", "YeoJohnson"))


t_pca
```
